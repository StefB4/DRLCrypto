{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5172c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os \n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# TODO\n",
    "\n",
    "# prices nicht skewen? \n",
    "# non negative balance wie an agent feedbacken? \n",
    "# wie sicherstellen, dass nur innerhalb des balance gekauft wird? \n",
    "# starten immer mit random timestep oder von anfang an \n",
    "# turbulence mit angepasster reward function \n",
    "# cut action von [-1,1]? necessary? \n",
    "# flatten observation? sonst lÃ¤uft stable baseline nicht \n",
    "\n",
    "PROCESSEDDATA = \"./../processeddata\"\n",
    "\n",
    "INITIAL_BALANCE = 2000\n",
    "# for normalization\n",
    "MAX_BALANCE = 1000000 \n",
    "MAX_PRICE = 10000 \n",
    "MAX_INDEX_VAL = 1000 \n",
    "MAX_HOLDING_VAL = 10000  \n",
    "MAX_BUYING_AMOUNT = 10000 \n",
    "TRANSACTION_PERCENTAGE_COST = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75d1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"hello from CryptoEnv\")\n",
    "    \n",
    "        super(CryptoEnv, self).__init__()\n",
    "        \n",
    "        self.crypto_data = [] # ['unixtime', 'interpolateddata', 'open', 'close', 'low', 'high', 'macd', 'rsi', 'cci', 'adx']\n",
    "        self.crypto_data_names = [] # ['ADA', 'ETH', 'XRP', 'XMR', 'LTC']\n",
    "        self._read_processed_data()\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-1,high=+1,shape=(len(self.crypto_data_names),), dtype=np.float32)\n",
    "        # for each crypto (normalized from -1 to +1): negative: sell, 0: hold, positive: buy \n",
    "        \n",
    "        obs_lows = [0] + [0] * len(self.crypto_data_names) * 2 + [-1] * len(self.crypto_data_names) + [0] * len(self.crypto_data_names) * 2 + [-1] * len(self.crypto_data_names)\n",
    "        obs_highs =[1] + [1] * len(self.crypto_data_names) * 6 \n",
    "        self.observation_space = spaces.Box(low = np.array(obs_lows), high = np.array(obs_highs), dtype=np.float32)\n",
    "        # all normalized from -1 to 1                                     \n",
    "        # balance (pure money) (>= 0, single number)\n",
    "        # amount owned of each crypto (>= 0, vector)\n",
    "        # close price of each crypto (>= 0, vector)\n",
    "        # MACD of each crypto (neg or pos, vector)\n",
    "        # RSI of each crypto (>= 0, vector)\n",
    "        # CCI of each crypto (>= 0, vector)\n",
    "        # ADX of each crypto (neg or pos, vector)\n",
    "        \n",
    "        \n",
    "        # init used variables, are reset in reset()\n",
    "        self.current_step = -1\n",
    "        self.current_balance = -2\n",
    "        self.current_holdings = np.ones((len(self.crypto_data_names),)) * -1\n",
    "        \n",
    "        # set in _observe_step()\n",
    "        self.current_observation = -1 \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _read_processed_data(self):\n",
    "        for crypto_file in glob.glob(PROCESSEDDATA + \"/*.csv\"):\n",
    "            self.crypto_data.append(pd.read_csv(os.path.join(crypto_file), index_col=0))\n",
    "            name = os.path.basename(crypto_file)\n",
    "            name = name.split(\"BTC\")[0]\n",
    "            self.crypto_data_names.append(name)\n",
    "        \n",
    "        # drop some unimportant stuff \n",
    "        for idx, _ in enumerate(self.crypto_data):\n",
    "            self.crypto_data[idx].drop(columns=[\"timestamp\",\"openbtc\",\"lowbtc\",\"highbtc\",\"closebtc\"], inplace = True)\n",
    "            self.crypto_data[idx].drop(columns=[\"high\",\"low\",\"open\",\"interpolateddata\"], inplace = True)\n",
    "    \n",
    "    \n",
    "    def _perform_action(self,action):\n",
    "        \n",
    "        \n",
    "        # Implementation:\n",
    "        # Sell individual holdings where possible \n",
    "        # Only buy all that are planned to be bought or none  \n",
    "        \n",
    "        \n",
    "        # De-normalize \n",
    "        action = action * MAX_BUYING_AMOUNT \n",
    "        \n",
    "        # total buy price\n",
    "        total_buy_price = 0\n",
    "        \n",
    "        \n",
    "        # performs sells if possible immediately, count all buys together to evaluate if possible \n",
    "        for idx, amount in enumerate(action):\n",
    "            \n",
    "            \n",
    "            # want to sell\n",
    "            if (amount < 0): \n",
    "                if (self.current_holdings[idx] < amount):  # less crypto available in holdings than wanted to sell \n",
    "                    pass # do not sell and keep balance\n",
    "                else: # crypto available\n",
    "                    self.current_holdings[idx] -= amount # sell, remove from holdings\n",
    "                    self.current_balance += (amount * self.current_observation[1][1][idx] * MAX_PRICE) * (1 - TRANSACTION_PERCENTAGE_COST / 100) # add money to balance, deduct transaction cost; get price from observation\n",
    "            \n",
    "            # in case of hold do nothing \n",
    "            \n",
    "            # want to buy \n",
    "            elif (amount > 0):\n",
    "                total_buy_price += amount * self.current_observation[1][1][idx] * MAX_PRICE # add price of wanted cryptos to total buy price \n",
    "        \n",
    "        \n",
    "        # when buys are possible, perform\n",
    "        if total_buy_price <= self.current_balance:\n",
    "            \n",
    "            for idx, amount in enumerate(action):\n",
    "                \n",
    "                if (amount > 0):\n",
    "                    self.current_holdings[idx] += amount # buy, add to holdings\n",
    "                    self.current_balance -= (amount * self.current_observation[1][1][idx] * MAX_PRICE) * (1 + TRANSACTION_PERCENTAGE_COST / 100) # remove money from balance, deduct transaction cost; get price from observation\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    def _calc_portfolio_value(self):\n",
    "        \n",
    "        #print(self.current_balance + np.squeeze(np.dot(self.current_holdings,np.squeeze(self.current_observation[1][1] * MAX_PRICE))))\n",
    "        return self.current_balance + np.squeeze(np.dot(self.current_holdings,np.squeeze(self.current_observation[1][1] * MAX_PRICE)))\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Log\n",
    "        if (self.current_step % 100 == 0):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Timestep: \" + str(self.current_step) + \" Datetime: \" + str(pd.to_datetime(self.crypto_data[0].loc[self.current_step]['unixtime'],unit='s')))\n",
    "            print(\"Current balance: \" + str(self.current_balance))\n",
    "            for idx, holding in enumerate(self.current_holdings):\n",
    "                print(self.crypto_data_names[idx] + \" holding: \" + str(holding) + \"\\t\" + self.crypto_data_names[idx] + \" price: \" + str(self.current_observation[1][1][idx] * MAX_PRICE) + \" normalized price: \" + str(self.current_observation[1][1][idx]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        portfolio_val_before = self._calc_portfolio_value()\n",
    "        \n",
    "        # Execute one time step within the environment\n",
    "        self._perform_action(action)\n",
    "        \n",
    "        portfolio_val_after = self._calc_portfolio_value()\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = False \n",
    "        \n",
    "        # Reached end of data\n",
    "        if self.current_step >= len(self.crypto_data[0]['close']): \n",
    "            print(\"Reached end of data\")\n",
    "            done = True\n",
    "            \n",
    "\n",
    "        reward = portfolio_val_after - portfolio_val_before # transaction costs are already included in the portfolio \n",
    "        \n",
    "        # Episode done if balance below 0 \n",
    "        if self.current_balance < 0:\n",
    "            print(\"Balance below 0\")\n",
    "            done = True \n",
    "        \n",
    "\n",
    "        obs = self._observe_step()\n",
    "        \n",
    "\n",
    "        return obs, reward, done, {}\n",
    "        \n",
    "    \n",
    "    \n",
    "         \n",
    "    def _observe_step(self):\n",
    "        \n",
    "        \n",
    "        crypto_info_frames = []\n",
    "        \n",
    "        # extract relevant data for all cryptos \n",
    "        for idx, _ in enumerate(self.crypto_data):            \n",
    "            crypto_info_frames.append(self.crypto_data[idx].iloc[self.current_step])\n",
    "        crypto_info_df = pd.concat(crypto_info_frames, axis=1)\n",
    "        crypto_info_df = crypto_info_df.T\n",
    "        crypto_info_np = crypto_info_df.to_numpy()\n",
    "        crypto_info_np = np.array(crypto_info_np[:,1:])\n",
    "        # column 0: close, 1: macd, 2: rsi, 3: cci, 4: adx \n",
    "        # \n",
    "        # debugging\n",
    "        #print(\"crypto_info_df:\")\n",
    "        #print(crypto_info_df)\n",
    "        #print(\"crypto_info_np (removed unix time) (column 0: close, 1: macd, 2: rsi, 3: cci, 4: adx):\")\n",
    "        #print(crypto_info_np)\n",
    "        \n",
    "        \n",
    "        # normalize \n",
    "        crypto_info_np[:,0] = crypto_info_np[:,0] / MAX_PRICE\n",
    "        crypto_info_np[:,1] = crypto_info_np[:,1] / MAX_INDEX_VAL\n",
    "        crypto_info_np[:,2] = crypto_info_np[:,2] / MAX_INDEX_VAL\n",
    "        crypto_info_np[:,3] = crypto_info_np[:,3] / MAX_INDEX_VAL\n",
    "        crypto_info_np[:,4] = crypto_info_np[:,4] / MAX_INDEX_VAL\n",
    "        # debugging\n",
    "        #print(\"crypto_info_np (normalized):\")\n",
    "        #print(crypto_info_np)\n",
    "        \n",
    "        # concat balance and prices and indices \n",
    "        observation = np.vstack([self.current_holdings / MAX_HOLDING_VAL,crypto_info_np.T])\n",
    "        observation = np.array([[self.current_balance / MAX_BALANCE], observation], dtype=object)\n",
    "        # observation make up \n",
    "        # [current balance]\n",
    "        # [ [current holdings 1, 2, 3, ...]\n",
    "        #   [ close 1, 2, 3, ... ] \n",
    "        #   [ macd 1, 2, 3, ... ]  \n",
    "        #   [ rsi 1, 2, 3, ... ] \n",
    "        #   [ cci 1, 2, 3, ... ] \n",
    "        #   [ adx 1, 2, 3, ... ] \n",
    "        # ]\n",
    "        #\n",
    "        # debugging\n",
    "        #print(\"Current balance: \" + str(self.current_balance) + \" Current balance normalized: \" + str(self.current_balance / MAX_BALANCE))\n",
    "        #print(\"Current holdings: \\n\" + str(self.current_holdings) + \"\\n Current holdings normalized: \\n\" + str(self.current_holdings / MAX_HOLDING_VAL))\n",
    "        #print(\"Full observation shape: \" + str(np.shape(observation)))\n",
    "        #print(\"Observation index 0 shape: \" + str(np.shape(observation[0])))\n",
    "        #print(\"Observation index 0 content:\")\n",
    "        #print(observation[0])\n",
    "        #print(\"Observation index 1 shape: \" + str(np.shape(observation[1])))\n",
    "        #print(\"Observation index 1 content: (row 0: current holding, 1: close, 2: macd, 3: rsi, 4: cci, 5: adx)\")\n",
    "        #print(observation[1])\n",
    "        \n",
    "        \n",
    "\n",
    "        self.current_observation = observation \n",
    "        \n",
    "        # make observation into single vector \n",
    "        observation = np.insert(observation[1].flatten(), 0, observation[0], axis=0)        \n",
    "\n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Reset to a random number from start until half of available data points \n",
    "        self.current_step = random.randint(\n",
    "            0, len(self.crypto_data[0]['close']) // 2)\n",
    "        \n",
    "        self.current_balance = INITIAL_BALANCE\n",
    "        self.current_holdings = np.zeros((len(self.crypto_data_names)),) # start with no cryptos \n",
    "        # current prices, macd, rsi, cci, adx are obtained from the data \n",
    "        \n",
    "        # Observe with the newly set values \n",
    "        observation = self._observe_step()\n",
    "        \n",
    "        return observation \n",
    "\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "               \n",
    "        # Render the environment to the screen\n",
    "        pass\n",
    "    \n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d22cad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from CryptoEnv\n"
     ]
    }
   ],
   "source": [
    "env = CryptoEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3987f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 7000 Datetime: 2018-09-18 19:00:00\n",
      "Current balance: 2000\n",
      "ADA holding: 0.0\tADA price: 0.06762046 normalized price: 6.762045999999999e-06\n",
      "ETH holding: 0.0\tETH price: 206.812734 normalized price: 0.020681273400000002\n",
      "XRP holding: 0.0\tXRP price: 0.31390262 normalized price: 3.1390262e-05\n",
      "XMR holding: 0.0\tXMR price: 111.476078 normalized price: 0.0111476078\n",
      "LTC holding: 0.0\tLTC price: 53.674133999999995 normalized price: 0.0053674134\n",
      "Balance below 0\n",
      "35.8168380074203\n",
      "Balance below 0\n",
      "3.2808432148885913\n",
      "Balance below 0\n",
      "0.6432175786540029\n",
      "Balance below 0\n",
      "0.04261627516007138\n",
      "Balance below 0\n",
      "10.775967398425564\n",
      "Balance below 0\n",
      "9.79291119996924\n",
      "Balance below 0\n",
      "14.038265723036602\n",
      "Balance below 0\n",
      "14.717772392556071\n",
      "Balance below 0\n",
      "26.989407802931964\n",
      "0.0022120694636669214\n",
      "Balance below 0\n",
      "89.01755037623457\n",
      "Balance below 0\n",
      "31.392823985777795\n",
      "Balance below 0\n",
      "33.42322418000549\n",
      "Balance below 0\n",
      "9.538510637125\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The parameter loc has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8fb09b123b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    278\u001b[0m     ) -> \"PPO\":\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         return super(PPO, self).learn(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_sde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_sde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi, latent_sde)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiagGaussianDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0maction_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ReAlly/lib/python3.8/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter loc has invalid values"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "\n",
    "# The algorithms require a vectorized environment to run\n",
    "env2 = DummyVecEnv([lambda: CryptoEnv()])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env2, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(2000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d70ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
